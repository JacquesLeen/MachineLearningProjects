{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",

   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 8 - Deep Learning and Neural Networks\n",
    "\n",
    "Replicate computation done by our brain with a Neural Network. Separate the two questions $\\rightarrow$ how does a NN WORK? and how does a NN LEARN?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.1 - The Neuron\n",
    "\n",
    "It is the basic block of the NN. They are creat in analogy with actual Neurons in our brain, we recreate those into a machine. But how?\n",
    "\n",
    "A neuron has got a body, dendrites and an axon.  Alone the neuron cant do much but a mln of them becomes very powerful. The dendrites are the receivers of the signal, and the axon passes the signals over to the next neuron through a synapse (which is actually a void space).\n",
    "\n",
    "In ML the neuron gets a signal from inputs (mre than one) and has sends an output signals. The input corresponds to the senses of a person that sends electrical impulses to the brain. The brain is \"just\" a black box in that sense. Inouts are independent variable for A SINGLE OBSERVATION and they have to be standardized or normalized.\n",
    "\n",
    "The output value can be continous, discrete or a categorical variable (represented through a dummy variable). A single observation in input, despite having a set of input values (for different indep. variables) becomes a single output observation. \n",
    "\n",
    "The connection between input and body is a synapse and it is associated to a weight $\\rightarrow$ the weights $w$ are what gets updated from the NN and they are the updating process is learning method through which the NN works.\n",
    "\n",
    "What does the body do?\n",
    "\n",
    "1- $  \\sum w_i x_i$ $\\rightarrow$ weighted sums of the inputs $x_i$\n",
    "\n",
    "2- $\\varphi ( \\sum w_i x_i ) $ $\\rightarrow$ applies an activation function and according to the value decides whether to pass or not the signal.\n",
    "\n",
    "3- The neuron eventually passes on the signal\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.2 - The Activation Function $\\varphi$\n",
    "\n",
    "We have ben mentioning the activation function $\\varphi$. What is that? it decised whether the singal gets passed through. 4 main type of activation function\n",
    "\n",
    "1 - Threshold function $\\varphi$: if the value is less then $x_t$ then $\\varphi(x) = 0$ else $\\varphi(x)=1$\n",
    "\n",
    "2 - Sigmoid $\\varphi$: $$\\varphi(x) = \\frac{1}{1 + \\exp(-x)} $$\n",
    "where $x$ is a shortcut for $\\sum w_i x_i$\n",
    "\n",
    "3 - Rectifier $\\varphi$: $$ \\varphi(x) = \\text{max}(x,0) $$\n",
    "\n",
    "4 - Hyperpolic Tangent function $\\varphi$: $$\\varphi(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{1- \\exp(-2x)}{1+ \\exp(-2x)} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {

   "source": [
    "## 8.3 - How does NN work?\n",
    "\n",
    "House price evaluation (case study) $\\rightarrow$ we pretend NN is already trained\n",
    "\n",
    "Under the assumption that NN is already trained we can cover what the NN does. Say we have different parameters (Area, bedrooms, Distance to city center...) which form the input layer. The output would be calculated by the Neuron by weighing the elements of the input \n",
    "\n",
    "$$\n",
    " \\text{output} = \\sum_i w_i \\text{input}_i\n",
    "$$\n",
    "\n",
    "The structure is simple enough such that many algorithms can be coded into that. If we add hidden layers to the Neural structure we see how the NN provides a better perfoming implementation. Hidden layers evaluate inputs assigning different weights to the input values. A single neuron can even be disregarding some input an be \"focused\" on some other. They perform then better calculation related to those input features alone and comprehend the correlation between those and the output.\n",
    "\n",
    "Say neuron $N_i$ works only with inputs $x_1, x_3, x_7$. This is an algorithm that specifically searches for correlations between those input and the ouput. it does not care about the other input. $N_i$ fires up only if the correlation between the $x_j$ it selects is strong enough. With many neurons working like that, the flexibility is increased.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.4 - How does NN learn?\n",
    "\n",
    "1) Hard coded learning\n",
    "\n",
    "2) Create a facility for the program in which we only specify the desired outcome feature, and the machine learns how to go from the input to the output\n",
    "\n",
    "Perceptron\n",
    "\n",
    "$$\n",
    "\\text{input}_1 ... \\text{input_n} \\rightarrow \\text{single layer} \\rightarrow \\text{output}\n",
    "$$\n",
    "\n",
    "Say we have input values that have been supplied to perceptron, we get an output $y_o$, we plot it on a chart and we confront it with the supposed value $y$. we then calculate the Cost Function:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2}(y_o - y)^2\n",
    "$$\n",
    "\n",
    "which is in this case the error we have in our prediction (there are many possibile C functions). Once we have this value we feed it back to the NN and the weight get updated. Our goal is to find the weights that minimize the Cost Function.\n",
    "\n",
    "What if we feed in several rows with input? We replicate the same perceptron for as many rows as we want. We then obtain a set $Y_o = \\{ y_o^1 ... y_o^n\\}$ and we confront that set with $Y= \\{y^1 ... y^n\\}$. The cost function becomes now\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2}\\sum_i (y_o^i - y^i)^2 \n",
    "$$\n",
    "\n",
    "The update procedure is done as before. Let remember that the NN was copied several times. So the update happens simmultaneously with the same values for all the copies. The goal is always the minimisation of the Cost Function.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.5 - Gradient Descent\n",
    "\n",
    "How we adjust the weights after back propagation? For a single row and a single neuron:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2} (y_o - y)^2\n",
    "$$\n",
    "\n",
    "the best way to find the minimum is to derive and solve the associated equation. In the case where we have several input and hidden layers we need to adjust for increase in dimensionality. Instead of just deriving we evaluate the gradient of the function\n",
    "\n",
    "$$\n",
    "\\nabla C (x_1...x_n) = \\sum_i \\frac{\\partial C}{\\partial x_i }\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.6 - Stochastic Gradient Descent\n",
    "\n",
    "Gradient descent requires the C function to be convex. Stochastic GD accounts for the scenarios in which the C function is not convex. In GD we take all the rows, then compute the $\\nabla$ and we then adjust the weights. In SGD we perform GD for each row separately, and adjusting every single weights after every iteration of GD. Stochastic GD avoids the possibility to find a local minimum, which results in a stop in the algorithm but in a non ideal output.\n",
    "\n",
    "\n",
    "NEURAL NETWORK AND DEEP LEARNING - MICHAEL NIELSEN 2015"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.7 - Backpropagation\n",
    "\n",
    "Backpropagation is the process that allows us to train the NN by adjusting the weights. It is as it explains which part of the error each element of the neural network is responsible for. So what happens during training?\n",
    "\n",
    "1- Random initialisation of weights to small number close to 0\n",
    "\n",
    "2- Input the first observation\n",
    "\n",
    "3- Forward propagation limited by the weights\n",
    "\n",
    "4- Comparison of prediction and actual value\n",
    "\n",
    "5- Backpropagation and update of the weights\n",
    "\n",
    "6- repeat 1 to 5 and update the weight each time or after a batch of observation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the problem here is to understand from the dataset how a set of clients of a bank is likely to leave the bank according to some features we have been able to measure\n",
    "\n",
    "#Import a dataset\n",
    "dataset = pd.read_csv('Part 8 - Deep Learning/Section 39 - Artificial Neural Networks (ANN)/Churn_Modelling.csv') #name of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}